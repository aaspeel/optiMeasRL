{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"#\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import estimator class\n",
    "from rnnEstimator import RnnEstimator\n",
    "\n",
    "from myUtils.sequences_treatment import *\n",
    "\n",
    "# import environment class\n",
    "from optimalIntermittency import OptimalIntermittency\n",
    "\n",
    "# import deer classes\n",
    "from deer.learning_algos.q_net_keras import MyQNetwork\n",
    "from deer.agent import NeuralAgent\n",
    "import deer.experiment.base_controllers as bc\n",
    "\n",
    "# import functions from Keras for the RNN\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, SimpleRNN#, Dropout, Embedding, Masking, Bidirectional\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# construct and train a Sequential RNN model with keras\n",
    "numberSamples=300\n",
    "T_train=10\n",
    "generatorType='random01'\n",
    "\n",
    "# generate sequences for training\n",
    "(obj_train,obs_train)=generateSequence(T_train,numberSamples=numberSamples,generatorType=generatorType)\n",
    "sigma=randomSigma(T_train,numberSamples=numberSamples,p0=0.2)\n",
    "obs_train_corrupted=corruptSequence_outOfRange(obs_train,sigma)\n",
    "\n",
    "n_dim_obs=np.shape(obs_train_corrupted)[2]\n",
    "n_dim_obj=np.shape(obj_train)[2]\n",
    "\n",
    "model=Sequential()\n",
    "model.add(LSTM(20,input_shape=(None,n_dim_obs),return_sequences=True))\n",
    "model.add(LSTM(20,return_sequences=True))\n",
    "model.add(Dense(n_dim_obj,activation=None))\n",
    "\n",
    "model.compile(optimizer='adam',loss='mean_squared_error')\n",
    "model.summary()\n",
    "\n",
    "# train the RNN\n",
    "history=model.fit(x=obs_train_corrupted,y=obj_train,batch_size=1,epochs=10,validation_split=0.2,verbose=2)\n",
    "\n",
    "# plot loss\n",
    "plotRNNresults(history)\n",
    "\n",
    "# plot prediction\n",
    "indToPlot=0\n",
    "obj_train_pred=model.predict(obs_train_corrupted[indToPlot:(indToPlot+1),:,:])\n",
    "\n",
    "pyplot.title('Prediction')\n",
    "pyplot.plot(obj_train[indToPlot,:,0], color='green', label='real')\n",
    "pyplot.plot(obj_train_pred[0,:,0], color='blue', label='pred')\n",
    "pyplot.plot(obs_train_corrupted[indToPlot,:,0], color='orange', label='measure corrupted')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct estimator\n",
    "est=RnnEstimator(model,generatorType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sequences\n",
    "T=T_train\n",
    "\n",
    "(objectives,observations)=est.generateSequence(T,numberSamples=numberSamples)\n",
    "\n",
    "print('shape objectives:',np.shape(objectives))\n",
    "print('shape observations:',np.shape(observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the RL policy (used for training)\n",
    "rng=np.random.RandomState(123456)\n",
    "env=OptimalIntermittency(est,objectives,observations,rng)\n",
    "qnetwork=MyQNetwork(environment=env,random_state=rng)\n",
    "agent=NeuralAgent(env,qnetwork,random_state=rng)\n",
    "\n",
    "# load agent\n",
    "#agent.setNetwork('../../myFolder/myModels/retrainedAgent_KF')\n",
    "\n",
    "# --- Bind controllers to the agent ---\n",
    "# Before every training epoch, we want to print a summary of the agent's epsilon, discount and \n",
    "# learning rate as well as the training epoch number.\n",
    "agent.attach(bc.VerboseController())\n",
    "\n",
    "# During training epochs, we want to train the agent after every action it takes.\n",
    "# Plus, we also want to display after each training episode (!= than after every training) the average bellman\n",
    "# residual and the average of the V values obtained during the last episode.\n",
    "agent.attach(bc.TrainerController())\n",
    "\n",
    "# All previous controllers control the agent during the epochs it goes through. However, we want to interleave a \n",
    "# \"test epoch\" between each training epoch (\"one of two epochs\", hence the periodicity=2). We do not want these \n",
    "# test epoch to interfere with the training of the agent, which is well established by the TrainerController, \n",
    "# EpsilonController and alike. Therefore, we will disable these controllers for the whole duration of the test \n",
    "# epochs interleaved this way, using the controllersToDisable argument of the InterleavedTestEpochController. \n",
    "# The value of this argument is a list of the indexes of all controllers to disable, their index reflecting in \n",
    "# which order they were added. Here, \"0\" is refering to the firstly attached controller, thus the \n",
    "# VerboseController; \"2\" refers to the thirdly attached controller, thus the LearningRateController; etc. The order \n",
    "# in which the indexes are listed is not important.\n",
    "# For each test epoch, we want also to display the sum of all rewards obtained, hence the showScore=True.\n",
    "# Finally, we want to call the summarizePerformance method of Toy_Env every [summarize_every] *test* epochs.\n",
    "agent.attach(bc.InterleavedTestEpochController(\n",
    "    id=0,\n",
    "    epoch_length=T,\n",
    "    controllers_to_disable=[1],\n",
    "    periodicity=2,\n",
    "    show_score=True,\n",
    "    summarize_every=1))\n",
    "\n",
    "print('Agent constructed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the RL agent\n",
    "agent.run(n_epochs=10, epoch_length=T)\n",
    "print('Agent Trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CORRUPT observations according to the agent policy\n",
    "(sigma,observations_corrupted_outOfRange)=corruptSequenceWithAgentPolicy(agent,observations)\n",
    "observations_corrupted_mask=corruptSequence_mask(observations,sigma)\n",
    "\n",
    "estimated_obj=est.estimateAll(observations_corrupted_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "ind_sample=1\n",
    "pyplot.title('Objective')\n",
    "pyplot.plot(estimated_obj[ind_sample,:], color='blue', label='filtered')\n",
    "pyplot.plot(objectives[ind_sample,:], color='orange', label='real')\n",
    "pyplot.plot(observations_corrupted_mask[ind_sample,:], color='red', label='obs_corrupted')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "absError=abs(objectives-estimated_obj)\n",
    "\n",
    "pyplot.title('Error')\n",
    "pyplot.plot(absError[ind_sample,:], color='blue', label='error')\n",
    "pyplot.plot(max(absError[ind_sample,:])*(1-sigma[ind_sample,:]), color='red', label='1-sigma')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
